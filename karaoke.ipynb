{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In karaoke, lyrics are presented on screen in rough synchronization with the appropriate portion of the song. However, unless manually done, the lyric guide (pointer/highlight over the appropriate word to be sung at a time) is absent. I attempt to automate this process.\n",
    "\n",
    "The high level idea is to use a pre-trained network (RNN?) on speech transcription, then use a similar idea to an image heatmap in CNNs to find the portion of the audio input that contributed most to the chosen word. With these time markers, we now have a lyric guide.\n",
    "\n",
    "Background: speech-to-text networks, CNN heatmaps\n",
    "\n",
    "Formally, this is known as the \"forced alignment problem\""
   ]
  },
  {
   "source": [
    "## Apporach 1: Using GRAD-CAM on CNN's\n",
    "\n",
    "#### For this, I will use end-to-end CNN's for ASR and apply the GRAD-CAM method described by Selvaraju et. al (2016) in [this paper](https://arxiv.org/abs/1610.02391). These approaches will follow a Connecitonist Temporal Classification (CTC) model.\n",
    "\n",
    "### Approach 1a: I will implement [this paper](https://arxiv.org/pdf/1701.02720.pdf) by Zhang et. al (2017) for the CNN."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_libri_speech_dataset(dataset_dir: str, dataset: str=\"train-clean-100\") -> torch.utils.data.Dataset:\n",
    "\n",
    "    \"\"\"\n",
    "    Function to download LibriSpeech dataset.\n",
    "\n",
    "    Inputs: \n",
    "    dataset_dir -- Path to directory where dataset should be located/downloaded\n",
    "    dataset -- Type of dataset desired. Options are \"train-clean-100\", \"train-clean-360\", \"train-clean-500\", \"dev-clean\", \"dev-other\", \"test-clean\", \"test-other\"\n",
    "    dataset\n",
    "\n",
    "    Output: torch.utils.data.Dataset of tuples with contents (waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id)\n",
    "    \"\"\"\n",
    "\n",
    "    # can use either key or url for \"url\" parameter of dataset download function\n",
    "    return torchaudio.datasets.LIBRISPEECH(dataset_dir, url=dataset, download=True)\n",
    "\n",
    "PATH_TO_DATASET_DIR = \"/mnt/d/Datasets/\"\n",
    "\n",
    "train_dataset = get_libri_speech_dataset(PATH_TO_DATASET_DIR)\n",
    "dev_dataset = get_libri_speech_dataset(PATH_TO_DATASET_DIR, \"dev-clean\")\n",
    "test_dataset = get_libri_speech_dataset(PATH_TO_DATASET_DIR, \"test-clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_from_waveform(waveform):\n",
    "\n",
    "    \"\"\"\n",
    "    Raw audio is transformed into 40-dimensional log mel-filter-bank (plus energy term) coefficients with deltas and delta-deltas, which reasults in 123 dimensional features. Each dimension is normalized to have zero mean and unit variance over the training set.\n",
    "\n",
    "    Basically this is just MFCC but without taking DCT at the end, but for the sake of cleanliness, I'll stick with MFCC for now. Also, I don't know what they mean by \"energy term\" (aren't the coefficients already energy terms?) so I'm omitting that.\n",
    "    \"\"\"\n",
    "\n",
    "    # Waveform has channel first dimension, gives shape [1, ...] which causes shape problems when stacking features\n",
    "    data = waveform.squeeze(dim=0)\n",
    "\n",
    "    # Grab desired features\n",
    "    # Takes in audio of dimension (..., time) returns specgram_mel of size (..., n_mfcc, time) where n_mfcc defaults to 40\n",
    "    mfcc_features = torchaudio.transforms.MFCC(log_mels=True)(data)\n",
    "    deltas = torchaudio.functional.compute_deltas(mfcc_features)\n",
    "    delta_deltas = torchaudio.functional.compute_deltas(deltas)\n",
    "\n",
    "    # Stack features together\n",
    "    input_features = torch.cat((mfcc_features, deltas, delta_deltas), 0)\n",
    "\n",
    "    # Normalize features along time dimension\n",
    "    # PyTorch built in Normalize transform does it to entire channels, we want along specific dimension of only channel, which we can do with LayerNorm Module\n",
    "\n",
    "    input_features_normalized = nn.LayerNorm(input_features.shape[1], elementwise_affine=False)(input_features)\n",
    "\n",
    "    # Old ways of experimenting with normalizing along dimensions. \n",
    "    \n",
    "    # Unsqueeze method\n",
    "    # input_features_normalized = (input_features - input_features.mean(dim=1).unsqueeze(dim=1)) / input_features.std(dim=1).unsqueeze(dim=1)\n",
    "\n",
    "    # Transpose method\n",
    "    # input_features_normalized = ((input_features.T - input_features.T.mean(0)) / input_features.T.std(0)).T\n",
    "\n",
    "    # Numpy method: Using a useful function numpy has but torch does not. Creates small floating point differences, probably from conversion to torch tensor from numpy array.\n",
    "    # input_features_normalized = torch.from_numpy(np.apply_along_axis(lambda x : (x - np.mean(x)) / np.std(x), 1, input_features))\n",
    "\n",
    "    return input_features_normalized\\\n",
    "\n",
    "def preprocess(dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocesses dataset\n",
    "\n",
    "    1. Convert waveforms to input features, pad for consistent sizes\n",
    "    2. TODO: Conver labels to something\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    input_lengths = []\n",
    "    output_lengths = []\n",
    "\n",
    "    # Each waveform has different lengths of time dimension, so we need to pad them. The built-in fn assumes trailing dimensions of all sequences are the same, so we have to transpose to pad the correct dimension since time dim (dim=1) is different, then transpose back after.\n",
    "\n",
    "    # Current output of features_from_waveform have dim: 120 x time\n",
    "    for waveform, _, transcript, _, _, _ in dataset:\n",
    "        inputs.append(features_from_waveform(waveform).transpose(0, 1))\n",
    "\n",
    "    # Returns dimensions: num_inputs (batch) x time x features, so transpose appropriately\n",
    "    # TODO; Need to add back (unsqueeze) channel dimension\n",
    "    inputs = nn.utils.rnn.pad_sequence(inputs, batch_first=True).transpose(1, 2) \n",
    "\n",
    "    return inputs  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dataloader stuff to handle batching datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxout():\n",
    "    pass\n",
    "\n",
    "class Conv_Maxout_Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, filter_dim=(3,5), pool=False, pool_dim=(3,1), dropout=0.3):\n",
    "        super(Conv_Maxout_Layer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=filter_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        # TODO: implement maxout activation\n",
    "        if pool:\n",
    "            x = nn.MaxPool2d(kernel_size=pool_dim)(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class FC_Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.3):\n",
    "        super(FC_Layer, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.fc(x)\n",
    "            # maxout\n",
    "            x = self.dropout(x)\n",
    "            return x\n",
    "\n",
    "class ASR_1(nn.Module):\n",
    "\n",
    "        \"\"\"\n",
    "        Unlike the other layers, the first convolutional layer is followed by a pooling layer, which is described in section 2. The pooling size is 3 × 1, which mean swe only pool over the frequency axis. The filter size is 3 × 5 across the layers. The model has 128 feature maps in the first four convolutional layers and 256 feature maps in the remaining six convolutional layers. Each fully-connected layer has 1024 units. Maxout with 2 piece-wise linear functions is used as the activation function.\n",
    "        \"\"\" \n",
    "\n",
    "        def __init__(self, in_dim, out_dim):\n",
    "            super(ASR_1, self).__init__()\n",
    "            # in_dim should be 40-dimensional from log MFCC + extra from deltas and delta-deltas = 123 total\n",
    "            # should just be equal to dimensions of input features\n",
    "            self.in_dim = in_dim \n",
    "            self.cnn_layers = nn.Sequential(\n",
    "                            Conv_Maxout_Layer(self.in_dim, 128, pool=True, dropout=0),\n",
    "                            Conv_Maxout_Layer(128, 128),\n",
    "                            Conv_Maxout_Layer(128, 128),\n",
    "                            Conv_Maxout_Layer(128, 128),\n",
    "                            Conv_Maxout_Layer(128, 256),\n",
    "                            Conv_Maxout_Layer(256, 256),\n",
    "                            Conv_Maxout_Layer(256, 256),\n",
    "                            Conv_Maxout_Layer(256, 256),\n",
    "                            Conv_Maxout_Layer(256, 256),\n",
    "                            Conv_Maxout_Layer(256, 256),\n",
    "            )\n",
    "\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                            FC_Layer(1, 1024),\n",
    "                            FC_Layer(1024, 1024),\n",
    "                            FC_Layer(1024, 1024),\n",
    "                            FC_Layer(1024, out_dim, dropout=0)\n",
    "            )\n",
    "\n",
    "            # self.conv1 = nn.Conv2d(self.in_dim, 128, (3, 5))\n",
    "            # self.pool = nn.MaxPool2d((3, 1))\n",
    "            # self.conv2 = nn.Conv2d(128, 128, (3, 5))\n",
    "            # self.conv3 = nn.Conv2d(128, 128, (3, 5))\n",
    "            # self.conv4 = nn.Conv2d(128, 128, (3, 5))\n",
    "            # self.conv5 = nn.Conv2d(128, 256, (3, 5))\n",
    "            # self.conv6 = nn.Conv2d(256, 256, (3, 5))\n",
    "            # self.conv7 = nn.Conv2d(256, 256, (3, 5))\n",
    "            # self.conv8 = nn.Conv2d(256, 256, (3, 5))\n",
    "            # self.conv9 = nn.Conv2d(256, 256, (3, 5))\n",
    "            # self.conv10 = nn.Conv2d(256, 256, (3, 5))\n",
    "            # first arg of fc1 should be w * h of input features * 256\n",
    "            # self.fc1 = nn.Linear(1, 1024)\n",
    "            # self.fc2 = nn.Linear(1024, 1024)\n",
    "            # need to add last layer for character classes (+ blank for CTC?)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.cnn_layers(x)\n",
    "            # transfomration from cnn dimensions to fc dimensions\n",
    "            x = self.fc_layers(x)\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'in_dim' and 'out_dim'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-00aa5f00d97e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mASR_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'in_dim' and 'out_dim'"
     ]
    }
   ],
   "source": [
    "net = ASR_1()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAM loss w/ lr=10e-4, batch size 20, initial weights initialized uniformly from [-0.05, 0.05], dropout w/ p=0.3 used in all layers except in and out\n",
    "# for fine tuning: SGD w/ lr 10e-5, l2 penalty w/ coeff=1e-5\n",
    "\n",
    "loss = nn.CTCLoss()(ouput, labels, input_lengths, label_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('lyricism': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4a7f85a872575b073bd90bf4615f69cb1d91d2877fa6e85aaf45a76ec5c896f5"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}