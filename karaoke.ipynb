{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In karaoke, lyrics are presented on screen in rough synchronization with the appropriate portion of the song. However, unless manually done, the lyric guide (pointer/highlight over the appropriate word to be sung at a time) is absent. I attempt to automate this process.\n",
    "\n",
    "The high level idea is to use a pre-trained network (RNN?) on speech transcription, then use a similar idea to an image heatmap in CNNs to find the portion of the audio input that contributed most to the chosen word. With these time markers, we now have a lyric guide.\n",
    "\n",
    "Background: speech-to-text networks, CNN heatmaps\n",
    "\n",
    "Formally, this is known as the \"forced alignment problem\""
   ]
  },
  {
   "source": [
    "## Apporach 1: Using GRAD-CAM on CNN's\n",
    "\n",
    "#### For this, I will use end-to-end CNN's for ASR and apply the GRAD-CAM method described by Selvaraju et. al (2016) in [this paper](https://arxiv.org/abs/1610.02391). These approaches will follow a Connecitonist Temporal Classification (CTC) model.\n",
    "\n",
    "### Approach 1a: I will implement [this paper](https://arxiv.org/pdf/1701.02720.pdf) by Zhang et. al (2017) for the CNN."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# TODO: Get CUDA working\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this function really necessary?\n",
    "\n",
    "def get_libri_speech_dataset(dataset_dir: str, dataset: str=\"train-clean-100\") -> torch.utils.data.Dataset:\n",
    "\n",
    "    \"\"\"\n",
    "    Function to download LibriSpeech dataset.\n",
    "\n",
    "    Inputs: \n",
    "    dataset_dir -- Path to directory where dataset should be located/downloaded\n",
    "    dataset -- Type of dataset desired. Options are \"train-clean-100\", \"train-clean-360\", \"train-clean-500\", \"dev-clean\", \"dev-other\", \"test-clean\", \"test-other\"\n",
    "    dataset\n",
    "\n",
    "    Output: torch.utils.data.Dataset of tuples with contents (waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id)\n",
    "    \"\"\"\n",
    "\n",
    "    # can use either key or url for \"url\" parameter of dataset download function\n",
    "    return torchaudio.datasets.LIBRISPEECH(dataset_dir, url=dataset, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_map_str = \"\"\"\n",
    " <SPACE> 1\n",
    " a 2\n",
    " b 3\n",
    " c 4\n",
    " d 5\n",
    " e 6\n",
    " f 7\n",
    " g 8\n",
    " h 9\n",
    " i 10\n",
    " j 11\n",
    " k 12\n",
    " l 13\n",
    " m 14\n",
    " n 15\n",
    " o 16\n",
    " p 17\n",
    " q 18\n",
    " r 19\n",
    " s 20\n",
    " t 21\n",
    " u 22\n",
    " v 23\n",
    " w 24\n",
    " x 25\n",
    " y 26\n",
    " z 27\n",
    " ' 28\n",
    " \"\"\"\n",
    "\n",
    "def create_char_map(char_map_str):\n",
    "    char_map = {}\n",
    "    for line in char_map_str.strip().split(\"\\n\"):\n",
    "        c, num = line.split()\n",
    "        char_map[c] = int(num)\n",
    "    return char_map\n",
    "\n",
    "char_map = create_char_map(char_map_str)\n",
    "\n",
    "def text_to_target(text, char_map):\n",
    "    target = []\n",
    "    for c in text:\n",
    "        if c == \" \":\n",
    "            target.append(char_map[\"<SPACE>\"])\n",
    "        else:\n",
    "            target.append(char_map[c])\n",
    "    return torch.Tensor(target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_from_waveform(waveform):\n",
    "\n",
    "    \"\"\"\n",
    "    Raw audio is transformed into 40-dimensional log mel-filter-bank (plus energy term) coefficients with deltas and delta-deltas, which reasults in 123 dimensional features. Each dimension is normalized to have zero mean and unit variance over the training set.\n",
    "\n",
    "    Basically this is just MFCC but without taking DCT at the end, but for the sake of cleanliness, I'll stick with MFCC for now. Also, I don't know what they mean by \"energy term\" (aren't the coefficients already energy terms?) so I'm omitting that.\n",
    "    \"\"\"\n",
    "\n",
    "    # Waveform has channel first dimension, gives shape [1, ...] which causes shape problems when stacking features\n",
    "    data = waveform.squeeze(dim=0)\n",
    "\n",
    "    # Grab desired features\n",
    "    # Takes in audio of dimension (..., time) returns specgram_mel of size (..., n_mfcc, time) where n_mfcc defaults to 40\n",
    "    mfcc_features = torchaudio.transforms.MFCC(log_mels=True)(data)\n",
    "    deltas = torchaudio.functional.compute_deltas(mfcc_features)\n",
    "    delta_deltas = torchaudio.functional.compute_deltas(deltas)\n",
    "\n",
    "    # Stack features together\n",
    "    input_features = torch.cat((mfcc_features, deltas, delta_deltas), 0)\n",
    "\n",
    "    # Normalize features along time dimension\n",
    "    # PyTorch built in Normalize transform does it to entire channels, we want along specific dimension of only channel, which we can do with LayerNorm Module\n",
    "\n",
    "    input_features_normalized = nn.LayerNorm(input_features.shape[1], elementwise_affine=False)(input_features)\n",
    "\n",
    "    # Old ways of experimenting with normalizing along dimensions. \n",
    "    \n",
    "    # Unsqueeze method\n",
    "    # input_features_normalized = (input_features - input_features.mean(dim=1).unsqueeze(dim=1)) / input_features.std(dim=1).unsqueeze(dim=1)\n",
    "\n",
    "    # Transpose method\n",
    "    # input_features_normalized = ((input_features.T - input_features.T.mean(0)) / input_features.T.std(0)).T\n",
    "\n",
    "    # Numpy method: Using a useful function numpy has but torch does not. Creates small floating point differences, probably from conversion to torch tensor from numpy array.\n",
    "    # input_features_normalized = torch.from_numpy(np.apply_along_axis(lambda x : (x - np.mean(x)) / np.std(x), 1, input_features))\n",
    "\n",
    "    return input_features_normalized\n",
    "\n",
    "def preprocess(dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocesses dataset\n",
    "\n",
    "    1. Convert waveforms to input features, pad for consistent sizes\n",
    "    2. TODO: Conver labels to something\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = [] # Features that are input to model\n",
    "    targets = [] # Target transcripts, NOT output of model, used for CTC Loss\n",
    "    # For these, CTCLoss expects them to be pre-padding\n",
    "    input_lengths = [] # Time lengths of input features\n",
    "    target_lengths = [] # Length of target transcripts\n",
    "\n",
    "    # Each waveform has different lengths of time dimension, so we need to pad them. The built-in fn assumes trailing dimensions of all sequences are the same, so we have to transpose to pad the correct dimension since time dim (dim=1) is different, then transpose back after.\n",
    "\n",
    "    # Current output of features_from_waveform have dim: 120 x time\n",
    "    for waveform, _, transcript, _, _, _ in dataset:\n",
    "        features = features_from_waveform(waveform).transpose(0, 1)\n",
    "        inputs.append(features)\n",
    "        input_lengths.append(features.shape[0])\n",
    "\n",
    "        target = text_to_target(transcript.lower(), char_map)\n",
    "        targets.append(target)\n",
    "        target_lengths.append(len(target))\n",
    "\n",
    "    # Returns dimensions: num_inputs (batch) x time x features, so transpose appropriately\n",
    "    # Need to add back (unsqueeze) channel dimension\n",
    "    # Side note: this will pad all samples within the same batch (not overall) to be the same dimensions\n",
    "    # TODO: Think about how this affects the dimensions of conv -> fc layers\n",
    "    inputs = nn.utils.rnn.pad_sequence(inputs, batch_first=True).unsqueeze(1).transpose(2, 3) \n",
    "\n",
    "    # This will pad with 0, which represents the blank, but this shouldn't be a problem since we're providing the target_length of the unpadded target\n",
    "    targets = nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "\n",
    "    return inputs, input_lengths, targets, target_lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dataloader stuff to handle batching datasets\n",
    "# Want DataLoader to take in raw dataset, batch them, pass into collate_fn to perform all necessary transformations, and return output\n",
    "# Output of form (feature_maps)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=preprocess)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxout():\n",
    "    # TODO: Implement this\n",
    "    pass\n",
    "\n",
    "class Conv_Maxout_Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, filter_dim=(3,5), pool=False, pool_dim=(3,1), dropout=0.3):\n",
    "        super(Conv_Maxout_Layer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=filter_dim)\n",
    "        # Using Dropout module vs functional will automatically disable it during model.eval()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.pool = pool\n",
    "        self.pool_dim = pool_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.pool:\n",
    "            x = nn.MaxPool2d(kernel_size=self.pool_dim)(x)\n",
    "        # maxout\n",
    "        # doing this after pooling has faster runtime because less operations\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class FC_Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.3):\n",
    "        super(FC_Layer, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        # maxout\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class ASR_1(nn.Module):\n",
    "\n",
    "        \"\"\"\n",
    "        Unlike the other layers, the first convolutional layer is followed by a pooling layer, which is described in section 2. The pooling size is 3 × 1, which mean swe only pool over the frequency axis. The filter size is 3 × 5 across the layers. The model has 128 feature maps in the first four convolutional layers and 256 feature maps in the remaining six convolutional layers. Each fully-connected layer has 1024 units. Maxout with 2 piece-wise linear functions is used as the activation function.\n",
    "        \"\"\" \n",
    "\n",
    "        def __init__(self, in_dim, out_dim):\n",
    "            super(ASR_1, self).__init__()\n",
    "            self.in_dim = in_dim \n",
    "            self.cnn_layers = nn.Sequential(\n",
    "                            Conv_Maxout_Layer(self.in_dim, 128, pool=True, dropout=0),\n",
    "                            Conv_Maxout_Layer(128, 128),\n",
    "                            Conv_Maxout_Layer(128, 128),\n",
    "                            Conv_Maxout_Layer(128, 128),\n",
    "                            Conv_Maxout_Layer(128, 256),\n",
    "                            Conv_Maxout_Layer(256, 256),\n",
    "                            Conv_Maxout_Layer(256, 256),\n",
    "                            Conv_Maxout_Layer(256, 256),\n",
    "                            Conv_Maxout_Layer(256, 256),\n",
    "                            Conv_Maxout_Layer(256, 256),\n",
    "            )\n",
    "            # TODO: Figure out dimension from last conv to first fc\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                            FC_Layer(1, 1024),\n",
    "                            FC_Layer(1024, 1024),\n",
    "                            FC_Layer(1024, 1024),\n",
    "                            FC_Layer(1024, out_dim, dropout=0)\n",
    "            )\n",
    "\n",
    "            # TODO: need to add last layer for character classes (+ blank for CTC?)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.cnn_layers(x)\n",
    "            # TODO: transfomration from cnn dimensions to fc dimensions\n",
    "            x = self.fc_layers(x)\n",
    "            # TODO: Add log softmax at end for CTCLoss to work:  torch.nn.functional.log_softmax()\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Old way of initializing weights by iterating through sub-modules rather than parameters directly\n",
    "# def weights_init_unif(m, a, b):\n",
    "#     if type(m) is nn.Linear or type(m) is nn.Conv2d:\n",
    "#         nn.init.uniform_(m.weight.data, a=a, b=b)\n",
    "#         nn.init.uniform_(m.bias.data, a=a, b=b)\n",
    "\n",
    "def weights_init_unif(module, a, b):\n",
    "    for p in module.parameters():\n",
    "        nn.init.uniform_(p.data, a=a, b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATASET_DIR = \"/mnt/d/Datasets/\"\n",
    "\n",
    "# def main():\n",
    "\n",
    "hparams = {\n",
    "    \"ADAM_lr\": 10e-4,\n",
    "    \"batch_size\": 20,\n",
    "    \"SGD_lr\": 10e-5,\n",
    "    \"SGD_l2_penalty\": 1e-5,\n",
    "    \"weights_init_a\": -0.05,\n",
    "    \"weights_init_b\": 0.05,\n",
    "    \"epochs\": 10\n",
    "}\n",
    "\n",
    "train_dataset = get_libri_speech_dataset(PATH_TO_DATASET_DIR)\n",
    "dev_dataset = get_libri_speech_dataset(PATH_TO_DATASET_DIR, \"dev-clean\")\n",
    "test_dataset = get_libri_speech_dataset(PATH_TO_DATASET_DIR, \"test-clean\")\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=preprocess)\n",
    "\n",
    "# dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=preprocess)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=preprocess)\n",
    "\n",
    "# 1 channel input from feature spectrogram, 29 dim output from char_map + blank for CTC\n",
    "net = ASR_1(1, 29)\n",
    "# net.apply(lambda x: weights_init_unif(x, hparams[\"weights_init_a\"], hparams[\"weights_init_b\"]))\n",
    "weights_init_unif(net, hparams[\"weights_init_a\"], hparams[\"weights_init_b\"])\n",
    "\n",
    "# ADAM loss w/ lr=10e-4, batch size 20, initial weights initialized uniformly from [-0.05, 0.05], dropout w/ p=0.3 used in all layers except in and out\n",
    "# for fine tuning: SGD w/ lr 10e-5, l2 penalty w/ coeff=1e-5\n",
    "\n",
    "criterion = nn.CTCLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=hparams[\"ADAM_lr\"])\n",
    "finetune_optimizer = torch.optim.SGD(net.parameters(), lr=hparams[\"SGD_lr\"], weight_decay=hparams[\"SGD_l2_penalty\"])\n",
    "\n",
    "# for epoch in range(hparams[\"epochs\"]):\n",
    "#     train(net, train_loader, criterion, optimizer)\n",
    "    \n",
    "# TODO: Where/when to do dev set?\n",
    "\n",
    "# test(net, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, criterion, optimizer):\n",
    "    model.train()\n",
    "    for inputs, input_lengths, targets, target_lengths in train_dataset:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # check what shape this is\n",
    "        output = model(inputs)\n",
    "\n",
    "        loss = criterion(output, targets, input_lengths, target_lengths)\n",
    "        loss.backwards()\n",
    "\n",
    "        optimizer.step() \n",
    "\n",
    "def test(model, test_dataset, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, input_lengths, targets, target_lengths in test_dataset:\n",
    "\n",
    "            output = model(inputs)\n",
    "\n",
    "            loss = criterion(output, targets, input_lengths, target_lengths)\n",
    "            \n",
    "            # TODO: Decoding algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('lyricism': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4a7f85a872575b073bd90bf4615f69cb1d91d2877fa6e85aaf45a76ec5c896f5"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}