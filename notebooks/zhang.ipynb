{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In karaoke, lyrics are presented on screen in rough synchronization with the appropriate portion of the song. However, unless manually done, the lyric guide (pointer/highlight over the appropriate word to be sung at a time) is absent. I attempt to automate this process.\n",
    "\n",
    "The high level idea is to use a pre-trained network (RNN?) on speech transcription, then use a similar idea to an image heatmap in CNNs to find the portion of the audio input that contributed most to the chosen word. With these time markers, we now have a lyric guide.\n",
    "\n",
    "Background: speech-to-text networks, CNN heatmaps\n",
    "\n",
    "Formally, this is known as the \"forced alignment problem\""
   ]
  },
  {
   "source": [
    "## Apporach 1: Using GRAD-CAM on CNN's\n",
    "\n",
    "#### For this, I will use end-to-end CNN's for ASR and apply the GRAD-CAM method described by Selvaraju et. al (2016) in [this paper](https://arxiv.org/abs/1610.02391). These approaches will follow a Connecitonist Temporal Classification (CTC) model.\n",
    "\n",
    "### Approach 1a: I will implement [this paper](https://arxiv.org/pdf/1701.02720.pdf) by Zhang et. al (2017) for the CNN."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this function really necessary?\n",
    "\n",
    "def get_libri_speech_dataset(dataset_dir: str, dataset: str=\"train-clean-100\") -> torch.utils.data.Dataset:\n",
    "\n",
    "    \"\"\"\n",
    "    Function to download LibriSpeech dataset.\n",
    "\n",
    "    Inputs: \n",
    "    dataset_dir -- Path to directory where dataset should be located/downloaded\n",
    "    dataset -- Type of dataset desired. Options are \"train-clean-100\", \"train-clean-360\", \"train-clean-500\", \"dev-clean\", \"dev-other\", \"test-clean\", \"test-other\"\n",
    "    dataset\n",
    "\n",
    "    Output: torch.utils.data.Dataset of tuples with contents (waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id)\n",
    "    \"\"\"\n",
    "\n",
    "    # can use either key or url for \"url\" parameter of dataset download function\n",
    "    return torchaudio.datasets.LIBRISPEECH(dataset_dir, url=dataset, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_map_str = \"\"\"\n",
    " <SPACE> 1\n",
    " a 2\n",
    " b 3\n",
    " c 4\n",
    " d 5\n",
    " e 6\n",
    " f 7\n",
    " g 8\n",
    " h 9\n",
    " i 10\n",
    " j 11\n",
    " k 12\n",
    " l 13\n",
    " m 14\n",
    " n 15\n",
    " o 16\n",
    " p 17\n",
    " q 18\n",
    " r 19\n",
    " s 20\n",
    " t 21\n",
    " u 22\n",
    " v 23\n",
    " w 24\n",
    " x 25\n",
    " y 26\n",
    " z 27\n",
    " ' 28\n",
    " \"\"\"\n",
    "\n",
    "def create_char_map(char_map_str):\n",
    "    char_map = {}\n",
    "    for line in char_map_str.strip().split(\"\\n\"):\n",
    "        c, num = line.split()\n",
    "        char_map[c] = int(num)\n",
    "    return char_map\n",
    "\n",
    "char_map = create_char_map(char_map_str)\n",
    "\n",
    "def text_to_target(text, char_map):\n",
    "    target = []\n",
    "    for c in text:\n",
    "        if c == \" \":\n",
    "            target.append(char_map[\"<SPACE>\"])\n",
    "        else:\n",
    "            target.append(char_map[c])\n",
    "    return torch.Tensor(target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_from_waveform(waveform):\n",
    "\n",
    "    \"\"\"\n",
    "    Raw audio is transformed into 40-dimensional log mel-filter-bank (plus energy term) coefficients with deltas and delta-deltas, which reasults in 123 dimensional features. Each dimension is normalized to have zero mean and unit variance over the training set.\n",
    "\n",
    "    Basically this is just MFCC but without taking DCT at the end, but for the sake of cleanliness, I'll stick with MFCC for now. Also, I don't know what they mean by \"energy term\" (aren't the coefficients already energy terms?) so I'm omitting that.\n",
    "    \"\"\"\n",
    "\n",
    "    # Waveform has channel first dimension, gives shape [1, ...] which causes shape problems when stacking features\n",
    "    data = waveform.squeeze(dim=0)\n",
    "\n",
    "    # Grab desired features\n",
    "    # Takes in audio of dimension (..., time) returns specgram_mel of size (..., n_mfcc, time) where n_mfcc defaults to 40\n",
    "    mfcc_features = torchaudio.transforms.MFCC(log_mels=True)(data)\n",
    "    deltas = torchaudio.functional.compute_deltas(mfcc_features)\n",
    "    delta_deltas = torchaudio.functional.compute_deltas(deltas)\n",
    "\n",
    "    # Stack features together\n",
    "    input_features = torch.cat((mfcc_features, deltas, delta_deltas), 0)\n",
    "\n",
    "    # Normalize features along time dimension\n",
    "    # PyTorch built in Normalize transform does it to entire channels, we want along specific dimension of only channel, which we can do with LayerNorm Module\n",
    "\n",
    "    input_features_normalized = nn.LayerNorm(input_features.shape[1], elementwise_affine=False)(input_features)\n",
    "\n",
    "    # Old ways of experimenting with normalizing along dimensions. \n",
    "    \n",
    "    # Unsqueeze method\n",
    "    # input_features_normalized = (input_features - input_features.mean(dim=1).unsqueeze(dim=1)) / input_features.std(dim=1).unsqueeze(dim=1)\n",
    "\n",
    "    # Transpose method\n",
    "    # input_features_normalized = ((input_features.T - input_features.T.mean(0)) / input_features.T.std(0)).T\n",
    "\n",
    "    # Numpy method: Using a useful function numpy has but torch does not. Creates small floating point differences, probably from conversion to torch tensor from numpy array.\n",
    "    # input_features_normalized = torch.from_numpy(np.apply_along_axis(lambda x : (x - np.mean(x)) / np.std(x), 1, input_features))\n",
    "\n",
    "    return input_features_normalized\n",
    "\n",
    "def preprocess(dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocesses dataset\n",
    "\n",
    "    1. Convert waveforms to input features\n",
    "    2. Convert transcripts to output class indices\n",
    "    3. Get input sequence (feature) lengths before padding\n",
    "    4. Get target lengths before padding\n",
    "    5. Pad inputs and targets for consistent sizes\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = [] # Features that are input to model\n",
    "    targets = [] # Target transcripts, NOT output of model, used for CTC Loss\n",
    "    # For these, CTCLoss expects them to be pre-padding\n",
    "    input_lengths = [] # Time lengths of input features\n",
    "    target_lengths = [] # Length of target transcripts\n",
    "\n",
    "    # Each waveform has different lengths of time dimension, so we need to pad them. The built-in fn assumes trailing dimensions of all sequences are the same, so we have to transpose to pad the correct dimension since time dim (dim=1) is different, then transpose back after.\n",
    "\n",
    "    for waveform, _, transcript, _, _, _ in dataset:\n",
    "        # Output of features_from_waveform have dim: 120 x time\n",
    "        features = features_from_waveform(waveform).transpose(0, 1)\n",
    "        inputs.append(features)\n",
    "        input_lengths.append(features.shape[0])\n",
    "\n",
    "        target = text_to_target(transcript.lower(), char_map)\n",
    "        targets.append(target)\n",
    "        target_lengths.append(len(target))\n",
    "\n",
    "    # Returns dimensions: num_inputs (batch) x time x features, so transpose appropriately\n",
    "    # Need to add back (unsqueeze) channel dimension\n",
    "    # Side note: this will pad all samples within the same batch (not overall) to be the same dimensions\n",
    "    # This transformation doesn't affect the model architecture since we are only flattening/connecting the feature dimension, not the time one (nn.Linear allows for this flexibility), so we can have variable length inputs across batches in this regard\n",
    "    inputs = nn.utils.rnn.pad_sequence(inputs, batch_first=True).unsqueeze(1).transpose(2, 3) \n",
    "\n",
    "    # This will pad with 0, which represents the blank, but this shouldn't be a problem since we're providing the target_length of the unpadded target\n",
    "    targets = nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "\n",
    "    return inputs, input_lengths, targets, target_lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel=(3,5), pool=False, pool_dim=(3,1), dropout=0.3):\n",
    "        super(Conv_Layer, self).__init__()\n",
    "\n",
    "        # Use padding so that feature maps don't decrease in size throughout network. Makes things wrt dimensions nice when moving to FC layers.\n",
    "        # Math says that we should pad by (1, 2) or equivalently (kernel_size[0]//2, kernel_size[1]//2) to keep dimensions the same\n",
    "        # However, Paper says they only pad along frame/time axis, so they assume dimensions of features will decrease? good for temporal nature of output we want, but more math for me! \n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel, padding=(0,2))\n",
    "        # self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel, padding=(0,2))\n",
    "        \n",
    "        # need to use module instead of functional since it has a parameter and needs to be pushed to cuda\n",
    "        # self.prelu = nn.PReLU(init=0.1)\n",
    "\n",
    "        # Using Dropout module vs functional will automatically disable it during model.eval()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Pool reduces dimension: ((old - (pool_dim - 1) - 1) / pool_dim) + 1, turns out to do 118 -> 39 in our case\n",
    "        self.pool = pool\n",
    "        self.pool_dim = pool_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        # Less optimal results, better memory usage\n",
    "        # x = self.prelu(x)\n",
    "        x = F.relu(x)\n",
    "        if self.pool:\n",
    "            x = nn.MaxPool2d(kernel_size=self.pool_dim)(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def conv_maxout(self, x):\n",
    "        # 2-way maxout creates 2 versions of a layer and maxes them, that's it\n",
    "        # Sadly I think will make the model too big to train for free locally or on Collab\n",
    "        \n",
    "        x1, x2 = self.conv1(x), self.conv2(x)\n",
    "        return torch.max(x1, x2)\n",
    "\n",
    "class FC_Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.3):\n",
    "        super(FC_Layer, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "        # self.fc2 = nn.Linear(in_dim, out_dim)\n",
    "        # self.prelu = nn.PReLU(init=0.1)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        # x = self.prelu(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def fc_maxout(self, x):\n",
    "        \n",
    "        x1, x2 = self.fc1(x), self.fc2(x)\n",
    "        return torch.max(x1, x2)\n",
    "\n",
    "class ASR_1(nn.Module):\n",
    "\n",
    "        \"\"\"\n",
    "        Unlike the other layers, the first convolutional layer is followed by a pooling layer, which is described in section 2. The pooling size is 3 × 1, which mean swe only pool over the frequency axis. The filter size is 3 × 5 across the layers. The model has 128 feature maps in the first four convolutional layers and 256 feature maps in the remaining six convolutional layers. Each fully-connected layer has 1024 units. Maxout with 2 piece-wise linear functions is used as the activation function.\n",
    "        \"\"\" \n",
    "\n",
    "        def __init__(self, in_dim, num_classes, num_features):\n",
    "            super(ASR_1, self).__init__()\n",
    "            self.in_dim = in_dim \n",
    "            self.cnn_layers = nn.Sequential(\n",
    "                            Conv_Layer(self.in_dim, 128, pool=True, dropout=0),\n",
    "                            Conv_Layer(128, 128),\n",
    "                            Conv_Layer(128, 128),\n",
    "                            Conv_Layer(128, 128),\n",
    "                            Conv_Layer(128, 256),\n",
    "                            Conv_Layer(256, 256),\n",
    "                            Conv_Layer(256, 256),\n",
    "                            Conv_Layer(256, 256),\n",
    "                            Conv_Layer(256, 256),\n",
    "                            Conv_Layer(256, 256),\n",
    "            )\n",
    "\n",
    "            # For feature maps, Conv2d layers keep time dimension the same, 3x5 conv decreases frequency dim by 2\n",
    "            #                   MaxPool2d layer only along frequency dimension, 3x1 pooling decreases frequency dim by 2\n",
    "            # Features dimenions of resulting feature map: num_features -> 39 (after first conv + pool) - 2 (conv[2-10]) * 9 = 39 - 18 = 21\n",
    "            # So in our use case, first fc layer should have dimensions = 256 * 21 = 5376\n",
    "\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                            FC_Layer(256 * 21, 1024),\n",
    "                            FC_Layer(1024, 1024),\n",
    "                            FC_Layer(1024, 1024),\n",
    "                            FC_Layer(1024, num_classes, dropout=0)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.cnn_layers(x)\n",
    "            # cnn output shape (batch_size, num_channels, num_features, time)\n",
    "            x = x.view(x.shape[0], x.shape[1] * x.shape[2], x.shape[3]) # flattens channels and feature dimensions into one\n",
    "            x = x.transpose(1, 2) # (batch_size, time, features)\n",
    "\n",
    "            # Input: (N, *, H_{in}) where * means any number of additional dimensions and H_in=in_features\n",
    "            x = self.fc_layers(x)\n",
    "\n",
    "            x = F.log_softmax(x, dim=2)\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Old way of initializing weights by iterating through sub-modules rather than parameters directly\n",
    "# net.apply(lambda x: weights_init_unif(x, hparams[\"weights_init_a\"], hparams[\"weights_init_b\"]))\n",
    "# def weights_init_unif(m, a, b):\n",
    "#     if type(m) is nn.Linear or type(m) is nn.Conv2d:\n",
    "#         nn.init.uniform_(m.weight.data, a=a, b=b)\n",
    "#         nn.init.uniform_(m.bias.data, a=a, b=b)\n",
    "\n",
    "def weights_init_unif(module, a, b):\n",
    "    for p in module.parameters():\n",
    "        nn.init.uniform_(p.data, a=a, b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epoch, device):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    for batch_num, data in enumerate(train_loader): \n",
    "        inputs, input_lengths, targets, target_lengths = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # this is shape (batch size, time, num_classes)\n",
    "        output = model(inputs)\n",
    "\n",
    "        # output passed in should be of shape (time, batch size, num_classes)\n",
    "        output = output.transpose(0, 1)\n",
    "        loss = criterion(output, targets, input_lengths, target_lengths)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step() \n",
    "\n",
    "        if batch_num % 100 == 0 or batch_num == data_len:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_num * len(inputs), data_len,\n",
    "                    100. * batch_num / len(train_loader), loss.item()))\n",
    "\n",
    "        save_path = CHECKPOINT_PATH + \"_epoch_\" + str(epoch) + \".pt\"\n",
    "\n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                    }, save_path)\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, input_lengths, targets, target_lengths in test_loader:\n",
    "\n",
    "            output = model(inputs)\n",
    "\n",
    "            loss = criterion(output, targets, input_lengths, target_lengths)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_DATASET_DIR = \"/mnt/d/Datasets/\"\n",
    "CHECKPOINT_PATH = \"./\"\n",
    "\n",
    "# def main():\n",
    "\n",
    "hparams = {\n",
    "    \"ADAM_lr\": 10e-4,\n",
    "    \"batch_size\": 20,\n",
    "    \"SGD_lr\": 10e-5,\n",
    "    \"SGD_l2_penalty\": 1e-5,\n",
    "    \"weights_init_a\": -0.05,\n",
    "    \"weights_init_b\": 0.05,\n",
    "    \"epochs\": 10\n",
    "}\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "train_dataset = get_libri_speech_dataset(PATH_TO_DATASET_DIR)\n",
    "dev_dataset = get_libri_speech_dataset(PATH_TO_DATASET_DIR, \"dev-clean\")\n",
    "test_dataset = get_libri_speech_dataset(PATH_TO_DATASET_DIR, \"test-clean\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=preprocess, pin_memory=use_cuda)\n",
    "\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=preprocess, pin_memory=use_cuda)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=preprocess, pin_memory=use_cuda)\n",
    "\n",
    "# 1 channel input from feature spectrogram, 29 dim output from char_map + blank for CTC, 120 features\n",
    "net = ASR_1(1, 29, 120)\n",
    "net.to(device)\n",
    "weights_init_unif(net, hparams[\"weights_init_a\"], hparams[\"weights_init_b\"])\n",
    "\n",
    "# ADAM loss w/ lr=10e-4, batch size 20, initial weights initialized uniformly from [-0.05, 0.05], dropout w/ p=0.3 used in all layers except in and out\n",
    "# for fine tuning: SGD w/ lr 10e-5, l2 penalty w/ coeff=1e-5\n",
    "\n",
    "criterion = nn.CTCLoss().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=hparams[\"ADAM_lr\"])\n",
    "finetune_optimizer = torch.optim.SGD(net.parameters(), lr=hparams[\"SGD_lr\"], weight_decay=hparams[\"SGD_l2_penalty\"])\n",
    "\n",
    "# for epoch in range(1, hparams[\"epochs\"] + 1):\n",
    "#     train(net, train_loader, criterion, optimizer, epoch, device)\n",
    "\n",
    "# test(net, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_checkpoint(model, optimizer, checkpoint_name, device):\n",
    "\n",
    "    path = CHECKPOINT_PATH + \"/\" + checkpoint_name\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    return model, optimizer, epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, _, _, _ = load_from_checkpoint(net, \"model_old_serial.pt\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}