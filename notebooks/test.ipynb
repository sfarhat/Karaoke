{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('karaoke': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c9cdb16bb67d9578cbd8e7b51d5e1a9b2b351d0fc74a15c7ef1e0879d6cc4541"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "# TODO: figure out how to modularize this nicely\n",
    "char_map_str = \"\"\"\n",
    " <SPACE> 1\n",
    " a 2\n",
    " b 3\n",
    " c 4\n",
    " d 5\n",
    " e 6\n",
    " f 7\n",
    " g 8\n",
    " h 9\n",
    " i 10\n",
    " j 11\n",
    " k 12\n",
    " l 13\n",
    " m 14\n",
    " n 15\n",
    " o 16\n",
    " p 17\n",
    " q 18\n",
    " r 19\n",
    " s 20\n",
    " t 21\n",
    " u 22\n",
    " v 23\n",
    " w 24\n",
    " x 25\n",
    " y 26\n",
    " z 27\n",
    " ' 28\n",
    " \"\"\"\n",
    "\n",
    "def create_char_map(char_map_str):\n",
    "    char_map, idx_map = {}, {}\n",
    "    for line in char_map_str.strip().split(\"\\n\"):\n",
    "        c, num = line.split()\n",
    "        char_map[c] = int(num)\n",
    "        idx_map[int(num)] = c\n",
    "    return char_map, idx_map\n",
    "\n",
    "char_map, idx_map = create_char_map(char_map_str)\n",
    "\n",
    "def text_to_target(text, char_map):\n",
    "    target = []\n",
    "    for c in text:\n",
    "        if c == \" \":\n",
    "            target.append(char_map[\"<SPACE>\"])\n",
    "        else:\n",
    "            target.append(char_map[c])\n",
    "    return torch.Tensor(target)\n",
    "\n",
    "def target_to_text(target):\n",
    "\n",
    "    text = \"\"\n",
    "    for idx in target:\n",
    "        idx = idx.item()\n",
    "        if idx == 1:\n",
    "            text += \" \"\n",
    "        else:\n",
    "            text += idx_map[idx]\n",
    "    return text\n",
    "\n",
    "def weights_init_unif(module, a, b):\n",
    "    for p in module.parameters():\n",
    "        nn.init.uniform_(p.data, a=a, b=b)\n",
    "\n",
    "def load_from_checkpoint(model, optimizer, checkpoint_name, device):\n",
    "\n",
    "    for root, dirs, files in os.walk('../'):\n",
    "        for filename in files:\n",
    "            if filename == checkpoint_name:\n",
    "                path = os.path.join(root, filename)\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    return model, optimizer, epoch, loss\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, activation, batch_size):\n",
    "\n",
    "    filename = \"activation-{}_batch-size-{}_epoch-{}.pt\".format(activation, batch_size, epoch)\n",
    "    save_path = CHECKPOINT_DIR + filename \n",
    "\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "                }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"/mnt/d/Datasets\"\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "\n",
    "hparams = {\n",
    "    \"ADAM_lr\": 10e-4,\n",
    "    \"batch_size\": 3,\n",
    "    \"SGD_lr\": 10e-5,\n",
    "    \"SGD_l2_penalty\": 1e-5,\n",
    "    \"weights_init_a\": -0.05,\n",
    "    \"weights_init_b\": 0.05,\n",
    "    \"epochs\": 10,\n",
    "    \"activation\": \"relu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "def features_from_waveform(waveform):\n",
    "\n",
    "    \"\"\"\n",
    "    Raw audio is transformed into 40-dimensional log mel-filter-bank (plus energy term) coefficients with deltas and delta-deltas, which reasults in 123 dimensional features.\n",
    "    Each dimension is normalized to have zero mean and unit variance over the training set.\n",
    "    Basically this is just MFCC but without taking DCT at the end, but for the sake of cleanliness, I'll stick with MFCC for now. \n",
    "    Also, I don't know what they mean by \"energy term\" (aren't the coefficients already energy terms?) so I'm omitting that.\n",
    "\n",
    "    :param waveform: Time series data representing spoken input. Shape (channel, amplitude, time)\n",
    "    :returns: \"Spectrogram\" of MFCC, delta, and delta-delta features. Shape (120, time)\n",
    "    \"\"\"\n",
    "\n",
    "    # Waveform has channel first dimension, gives shape (1, ...) which causes shape problems when stacking features\n",
    "    data = waveform.squeeze(dim=0)\n",
    "\n",
    "    # Grab desired features\n",
    "    mfcc_features = torchaudio.transforms.MFCC(log_mels=True)(data) # Takes in audio of dimension (..., time) returns (..., n_mfcc, time) where n_mfcc defaults to 40\n",
    "    deltas = torchaudio.functional.compute_deltas(mfcc_features)\n",
    "    delta_deltas = torchaudio.functional.compute_deltas(deltas)\n",
    "\n",
    "    # Stack features together\n",
    "    input_features = torch.cat((mfcc_features, deltas, delta_deltas), 0)\n",
    "\n",
    "    # Normalize (0 mean, 1 std) features along time dimension\n",
    "    input_features_normalized = nn.LayerNorm(input_features.shape[1], elementwise_affine=False)(input_features)\n",
    "\n",
    "    return input_features_normalized\n",
    "\n",
    "def preprocess(dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocesses dataset\n",
    "\n",
    "    1. Convert waveforms to input features\n",
    "    2. Convert transcripts to output class indices\n",
    "    3. Get input sequence (feature) lengths before padding\n",
    "    4. Get target lengths before padding\n",
    "    5. Pad inputs and targets for consistent sizes\n",
    "\n",
    "    This is fed into the DataLoader as the collate_fn, so keep in mind batch dimension.\n",
    "\n",
    "    :param dataset: Batched raw waveforms with shape (batch_size, channel, amplitude, time)\n",
    "    :returns inputs: Input \"filter banks\" to network (batch_size, num_channels, num_features, time). Padded along time dimension.\n",
    "    :returns input_lengths: Python list of respective lengths (time) of inputs in batch \n",
    "    :returns targets: Transcript of sound converted to Tensor of integers instead of characters. Padded for uniform lengths within batch.\n",
    "    :returns target_lengths: Python list of respective lenghts of transcripts in batch\n",
    "    \"\"\"\n",
    "\n",
    "    # These are necessary for CTCLoss\n",
    "    inputs = [] \n",
    "    targets = [] \n",
    "    # For these, CTCLoss expects them to be pre-padding\n",
    "    input_lengths = [] \n",
    "    target_lengths = [] \n",
    "\n",
    "    # Each waveform has different lengths of time dimension, so we need to pad them. \n",
    "    # The built-in fn assumes trailing dimensions of all sequences are the same, so we have to transpose to pad the correct dimension since time (dim=1) varies, then transpose back after.\n",
    "\n",
    "    for waveform, _, transcript, _, _, _ in dataset:\n",
    "        # Output of features_from_waveform have dim: (120, time)\n",
    "        features = features_from_waveform(waveform).transpose(0, 1)\n",
    "        inputs.append(features)\n",
    "        input_lengths.append(features.shape[0])\n",
    "\n",
    "        target = text_to_target(transcript.lower(), char_map)\n",
    "        targets.append(target)\n",
    "        target_lengths.append(len(target))\n",
    "\n",
    "    # Need to add back (unsqueeze) channel dimension\n",
    "    # Returns dimensions: (batch, time, features), so transpose appropriately\n",
    "    # Side note: this will pad all samples within the same batch (not overall) to be the same dimensions\n",
    "    # This transformation doesn't affect the model architecture since we are only flattening/connecting the feature dimension, not the time one (nn.Linear allows for this flexibility), so we can have variable length inputs across batches in this regard\n",
    "    inputs = nn.utils.rnn.pad_sequence(inputs, batch_first=True).unsqueeze(1).transpose(2, 3) \n",
    "\n",
    "    # This will pad with 0, which represents the blank, but this shouldn't be a problem since we're providing the target_length of the unpadded target\n",
    "    targets = nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "\n",
    "    return inputs, input_lengths, targets, target_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv_Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel, activation, dropout, pool=None, stride=1):\n",
    "        super(Conv_Layer, self).__init__()\n",
    "\n",
    "        # Use padding so that feature maps don't decrease in size throughout network. Makes things wrt dimensions nice when moving to FC layers.\n",
    "        # Math says that we should pad by (kernel_size[0]//2, kernel_size[1]//2) to keep dimensions the same, in our case this is (1, 2)  \n",
    "        # However, paper says they only pad along time axis, so number of features will decrease \n",
    "        self.kernel = kernel\n",
    "        self.padding = (0, self.kernel[1]//2)\n",
    "        self.stride = stride\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=self.kernel, padding=self.padding, stride=self.stride)\n",
    "\n",
    "        self.maxout = activation == \"maxout\"\n",
    "        if activation == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == \"prelu\":\n",
    "            # Need to use module instead of functional since it has a parameter and needs to be pushed to cuda\n",
    "            self.activation = nn.PReLU(init=0.1)\n",
    "        elif activation == \"maxout\":\n",
    "            # Better results (theoretically), worse memory usage\n",
    "            self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel, padding=(0,kernel[1]//2))\n",
    "        else:\n",
    "            raise Exception(\"Not a supported activation function. Choose between 'relu', 'prelu', or 'maxout'.\")\n",
    "       \n",
    "        # Using Dropout module vs functional will automatically disable it during model.eval()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Pool reduces dimension: ((old - (pool_dim - 1) - 1) / pool_dim) + 1, turns out to do 118 -> 39 in our case\n",
    "        self.pool = pool\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.maxout:\n",
    "            x = self.conv_maxout(x)\n",
    "        else:\n",
    "            x = self.conv(x)\n",
    "            x = self.activation(x)\n",
    "\n",
    "        if self.pool:\n",
    "            x = nn.MaxPool2d(kernel_size=self.pool)(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def conv_maxout(self, x):\n",
    "        # 2-way maxout creates 2 versions of a layer and maxes them, that's it\n",
    "        \n",
    "        x1, x2 = self.conv(x), self.conv2(x)\n",
    "        return torch.maximum(x1, x2)\n",
    "\n",
    "    def get_new_feature_map_dimension(self, in_dim):\n",
    "        # Convolution layer + operations will decrease the dimensions of the computed feature maps.\n",
    "        # This is important for figuring out the input dimensions of the first FC layer\n",
    "        # In our case, we only care about the feature (0th) dimension\n",
    "        # out = ((in + 2 * padding - kernel) // stride) + 1\n",
    "        # in pooling layer, stride is defaulted to kernel\n",
    "\n",
    "\n",
    "        # For feature maps, Conv2d: 3x5 conv (with appropriate padding) will decrease frequency dim by 2\n",
    "        # MaxPool2d: 3x1 pooling decreases frequency dim by 2\n",
    "        out = (in_dim + 2 * self.padding[0] - self.kernel[0]) // self.stride + 1\n",
    "        if self.pool:\n",
    "            out = (out - self.pool[0]) // self.pool[0] + 1\n",
    "            \n",
    "        return out  \n",
    "\n",
    "class FC_Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, activation, dropout):\n",
    "        super(FC_Layer, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "        \n",
    "        self.maxout = activation == \"maxout\"\n",
    "        if activation == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == \"prelu\":\n",
    "            # Need to use module instead of functional since it has a parameter and needs to be pushed to cuda\n",
    "            self.activation = nn.PReLU(init=0.1)\n",
    "        elif activation == \"maxout\":\n",
    "            # Better results (theoretically), worse memory usage\n",
    "            self.fc2 = nn.Linear(in_dim, out_dim)\n",
    "        else:\n",
    "            raise Exception(\"Not a supported activation function. Choose between 'relu', 'prelu', or 'maxout'.\")\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.maxout:\n",
    "            x = self.fc_maxout(x)\n",
    "        else:\n",
    "            x = self.fc(x)\n",
    "            x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def fc_maxout(self, x):\n",
    "        \n",
    "        x1, x2 = self.fc(x), self.fc2(x)\n",
    "        return torch.maximum(x1, x2)\n",
    "\n",
    "class ASR_1(nn.Module):\n",
    "\n",
    "        \"\"\"\n",
    "        Unlike the other layers, the first convolutional layer is followed by a pooling layer. \n",
    "        The pooling size is 3 × 1, which means we only pool over the frequency axis. The filter size is 3 × 5 across the layers. \n",
    "        The model has 256 feature maps in the first four convolutional layers and 256 feature maps in the remaining six convolutional layers. \n",
    "        Each fully-connected layer has 1024 units. Maxout with 2 piece-wise linear functions is used as the activation function.\n",
    "\n",
    "        We differ from the given model by using ReLU instead of Maxout for the activation function. This is simply due to memory constraints on \n",
    "        available hardware. \n",
    "        \"\"\" \n",
    "\n",
    "        def __init__(self, in_dim, num_classes, num_features, activation, dropout):\n",
    "            \n",
    "            super(ASR_1, self).__init__()\n",
    "\n",
    "            self.cnn_layers = nn.Sequential(\n",
    "                            Conv_Layer(in_channels=in_dim, out_channels=128, kernel=(3,5), activation=\"relu\", dropout=0, pool=(3,1)),\n",
    "                            Conv_Layer(128, 128, (3,5), activation, dropout),\n",
    "                            Conv_Layer(128, 128, (3,5), activation, dropout),\n",
    "                            Conv_Layer(128, 128, (3,5), activation, dropout),\n",
    "                            Conv_Layer(128, 256, (3,5), activation, dropout),\n",
    "                            Conv_Layer(256, 256, (3,5), activation, dropout),\n",
    "                            Conv_Layer(256, 256, (3,5), activation, dropout),\n",
    "                            Conv_Layer(256, 256, (3,5), activation, dropout),\n",
    "                            Conv_Layer(256, 256, (3,5), activation, dropout),\n",
    "                            Conv_Layer(256, 256, (3,5), activation, dropout)\n",
    "            )\n",
    "\n",
    "            # Features dimenion of resulting feature map: num_features = 120 -> 39 (after first conv + pool) - 2 (conv[2-10]) * 9 = 39 - 18 = 21\n",
    "            # So in our use case, first fc layer should have dimensions = 256 * 21 = 5376\n",
    "\n",
    "            fc_in = num_features\n",
    "            for layer in self.cnn_layers.children():\n",
    "                fc_in = layer.get_new_feature_map_dimension(fc_in)\n",
    "\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                            FC_Layer(in_dim=256 * fc_in, out_dim=1024, activation=activation, dropout=dropout),\n",
    "                            FC_Layer(1024, 1024, activation, dropout),\n",
    "                            FC_Layer(1024, 1024, activation, dropout),\n",
    "                            FC_Layer(1024, num_classes, activation, 0)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.cnn_layers(x) # output shape (batch_size, channels, features, time)\n",
    "            x = x.view(x.shape[0], x.shape[1] * x.shape[2], x.shape[3]) # flattens channel and feature dimensions into one\n",
    "            x = x.transpose(1, 2) # (batch_size, time, flattened_features)\n",
    "\n",
    "            x = self.fc_layers(x) # FC layer input: (N, *, H_{in}) where * means any number of additional dimensions and H_in=in_features\n",
    "\n",
    "            x = F.log_softmax(x, dim=2)\n",
    "            return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, epoch, device):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    for batch_num, data in enumerate(train_loader): \n",
    "        inputs, input_lengths, targets, target_lengths = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # this is shape (batch size, time, num_classes)\n",
    "        output = model(inputs)\n",
    "\n",
    "        # output passed in should be of shape (time, batch size, num_classes)\n",
    "        output = output.transpose(0, 1)\n",
    "        loss = criterion(output, targets, input_lengths, target_lengths)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step() \n",
    "\n",
    "        if batch_num % 100 == 0 or batch_num == data_len:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_num * len(inputs), data_len,\n",
    "                    100. * batch_num / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, input_lengths, targets, target_lengths in test_loader:\n",
    "\n",
    "            output = model(inputs)\n",
    "            output = output.transpose(0, 1)\n",
    "            loss = criterion(output, targets, input_lengths, target_lengths)\n",
    "            \n",
    "            # TODO: Decoding algo\n",
    "            # Transpose back so that we can iterate over batch dimension\n",
    "            output = output.transpose(0, 1)\n",
    "            for log_probs in output:\n",
    "                guessed_target = greedy_decode(torch.argmax(log_probs, dim=1))\n",
    "\n",
    "def greedy_decode(log_probs):\n",
    "\n",
    "    # TODO: incorporate target_length in here to reduce amount of work necessary\n",
    "    transcript = []\n",
    "    blank_seen = False\n",
    "    prev = None\n",
    "    for i in log_probs:\n",
    "        if i == prev and not blank_seen:\n",
    "           continue\n",
    "        elif i == 0:\n",
    "            blank_seen = True\n",
    "        else:\n",
    "            transcript.append(i)\n",
    "            blank_seen = False\n",
    "\n",
    "    return target_to_text(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"235.757344pt\" version=\"1.1\" viewBox=\"0 0 380.482813 235.757344\" width=\"380.482813pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-03-08T21:48:17.348862</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 235.757344 \nL 380.482813 235.757344 \nL 380.482813 0 \nL -0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 38.482813 211.879219 \nL 373.282813 211.879219 \nL 373.282813 10.999219 \nL 38.482813 10.999219 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p5ba5a63068)\">\n    <image height=\"201\" id=\"imageb6c41dbd81\" transform=\"scale(1 -1)translate(0 -201)\" width=\"335\" x=\"38.482813\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAU8AAADJCAYAAACjU7CDAAADLElEQVR4nO3WoU1lUQBF0ff/gCIUQDCEQVAB6E/oYhoglIGgiUlGIGmCDsCPQY1AIDEg3vSwzb0ka1Vw1M7Z7K7u14VlWZblx9Pz6AnT2Ds9GT1hGm+7o9ETpnFx8zJ6wjS2owcAfEfiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQbB7+Xq6jR8zi1+H76AnTOP99O3rCNPY/Ri+Yx/Zr9IJ5eJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgSbs8e7dfSIWayvB6MnTGP7OXrBPH7++Td6wjTero9HT5iG5wkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQCCeAIF4AgTiCRCIJ0AgngCBeAIE4gkQiCdAIJ4AgXgCBOIJEIgnQPAfYrEXa5pq6e8AAAAASUVORK5CYII=\" y=\"-10.879219\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mb52715e12e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"71.962813\" xlink:href=\"#mb52715e12e\" y=\"211.879219\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(68.781563 226.477656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"138.922813\" xlink:href=\"#mb52715e12e\" y=\"211.879219\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <g transform=\"translate(135.741563 226.477656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"205.882813\" xlink:href=\"#mb52715e12e\" y=\"211.879219\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2 -->\n      <g transform=\"translate(202.701563 226.477656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"272.842813\" xlink:href=\"#mb52715e12e\" y=\"211.879219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 3 -->\n      <g transform=\"translate(269.661563 226.477656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.802813\" xlink:href=\"#mb52715e12e\" y=\"211.879219\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 4 -->\n      <g transform=\"translate(336.621563 226.477656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m8d1e243c0b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m8d1e243c0b\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- −0.5 -->\n      <g transform=\"translate(7.2 14.798438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m8d1e243c0b\" y=\"44.479219\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.0 -->\n      <g transform=\"translate(15.579688 48.278437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m8d1e243c0b\" y=\"77.959219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.5 -->\n      <g transform=\"translate(15.579688 81.758437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m8d1e243c0b\" y=\"111.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 1.0 -->\n      <g transform=\"translate(15.579688 115.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m8d1e243c0b\" y=\"144.919219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.5 -->\n      <g transform=\"translate(15.579688 148.718437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m8d1e243c0b\" y=\"178.399219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 2.0 -->\n      <g transform=\"translate(15.579688 182.198438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m8d1e243c0b\" y=\"211.879219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 2.5 -->\n      <g transform=\"translate(15.579688 215.678438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 38.482813 211.879219 \nL 38.482813 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 373.282813 211.879219 \nL 373.282813 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 38.482813 211.879219 \nL 373.282813 211.879219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 38.482813 10.999219 \nL 373.282813 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p5ba5a63068\">\n   <rect height=\"200.88\" width=\"334.8\" x=\"38.482813\" y=\"10.999219\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# todo: get cuda working\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# train_dataset = torchaudio.datasets.LIBRISPEECH(DATASET_DIR, url=\"train-clean-100\", download=True)\n",
    "# dev_dataset = torchaudio.datasets.LIBRISPEECH(DATASET_DIR, url=\"dev-clean\", download=True)\n",
    "test_dataset = torchaudio.datasets.LIBRISPEECH(DATASET_DIR, url=\"test-clean\", download=True)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=preprocess, pin_memory=use_cuda)\n",
    "\n",
    "# dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=preprocess, pin_memory=use_cuda)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=preprocess, pin_memory=use_cuda)\n",
    "\n",
    "# 1 channel input from feature spectrogram, 29 dim output from char_map + blank for CTC, 120 features\n",
    "net = ASR_1(in_dim=1, num_classes=29, num_features=120, activation=hparams[\"activation\"], dropout=0.3)\n",
    "net.to(device)\n",
    "weights_init_unif(net, hparams[\"weights_init_a\"], hparams[\"weights_init_b\"])\n",
    "\n",
    "# ADAM loss w/ lr=10e-4, batch size 20, initial weights initialized uniformly from [-0.05, 0.05], dropout w/ p=0.3 used in all layers except in and out\n",
    "# for fine tuning: SGD w/ lr 10e-5, l2 penalty w/ coeff=1e-5\n",
    "\n",
    "criterion = nn.CTCLoss().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=hparams[\"ADAM_lr\"])\n",
    "finetune_optimizer = torch.optim.SGD(net.parameters(), lr=hparams[\"SGD_lr\"], weight_decay=hparams[\"SGD_l2_penalty\"])\n",
    "\n",
    "# for epoch in range(1, hparams[\"epochs\"] + 1):\n",
    "#     train(net, train_loader, criterion, optimizer, epoch, device)\n",
    "#     save_checkpoint(net, optimizer, epoch, hparams[\"activation\"], hparams[\"batch_size\"])\n",
    "\n",
    "    \n",
    "# TODO: Where/when to do dev set?\n",
    "\n",
    "net, _, _, _ = load_from_checkpoint(net, optimizer, \"activation-relu_batch-size-3_epoch-3.pt\", device)\n",
    "\n",
    "pass\n",
    "filter_list = list(net.children())[0][0].conv.weight.data\n",
    "im = filter_list[0]\n",
    "im = np.transpose(im, (1,2,0))\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Invalid shape (3, 5, 128) for image data",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-41ad7aa54e3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/karaoke/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m         data=None, **kwargs):\n\u001b[0;32m-> 2724\u001b[0;31m     __ret = gca().imshow(\n\u001b[0m\u001b[1;32m   2725\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/karaoke/lib/python3.9/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/karaoke/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5521\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5523\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5524\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/karaoke/lib/python3.9/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    709\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    710\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 711\u001b[0;31m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[1;32m    712\u001b[0m                             .format(self._A.shape))\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (3, 5, 128) for image data"
     ]
    }
   ],
   "source": [
    "filter_list = list(net.children())[0][1].conv.weight.data\n",
    "im = filter_list[0]\n",
    "im = np.transpose(im, (1,2,0))\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}